# Neural Networks and Deep Learning
 - ReLu: Rectified Linear Unit
 - For sequence data -> RNN (one-dimensional sequence data, temporal)

## Scale drives deep learning progress
 - Large amount of data
 - Large NN (train) (computation)
 - Algorithms
  - e.g. just by switching to the sigmoid function to the ReLu function has made an algorithm called gradient descent work much faster

