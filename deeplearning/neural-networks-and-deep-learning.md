# Neural Networks and Deep Learning
 - ReLu: Rectified Linear Unit
 - For sequence data -> RNN (one-dimensional sequence data, temporal)
 - `m` denotes the number of training examples (data)

## Scale drives deep learning progress
 - Large amount of data
 - Large NN (train) (computation)
 - Algorithms
  - e.g. just by switching to the sigmoid function to the ReLu function has made an algorithm called gradient descent work much faster

## Geoffrey Hinton interview
Back in the 50s, people like Von Neumann and Turing didn't believe in symbolic AI, they were far more inspired by the brain.
And in the early days of AI, people were completely convinced that the representations you need for intelligence were symbolic expressions of some kind.
Sort of cleaned-up logic, where you could do non-monotonic things, and not quite logic, but something like logic, and that the essence of intelligence was reasoning.
What's happened now is, there's a completely different view, which is that what a thought is just a great big vector of neural activity, so contrast that with a thought being a symbolic expression. And I think the people who thought that thoughts were symbolic expression just made a huge mistake.
...
So I think thoughts are just these great big vectors, and that big vectors have causal powers. They cause other big vectors, and that's utterly unlike the standard AI view that thoughts are symbolic expressions.
