\chapter{Prerequisites}
\begin{chapquote}{H.G. Rice [1953], \textit{paraphrased by Anders Moller}}
  ``Everything interesting about the behaviour of programs is
  undecidable.''
\end{chapquote}

The goal of \textit{static program analysis} is to verify certain
\textsl{properties} (or \textsl{behaviours}, or
\textsl{specifications}, or \textsl{statements}, ...) of the target
program \textbf{without its execution}.

For program \textsl{P} and property \textsl{S},

\begin{itemize}
\item $ \SEM{P} $: Formal semantics of program \textsl{P}.

\item \textsl{S}: Semantic properties that we're interested in. This
  could be defined in various level, such as ``Division-by-zero will
  \textbf{never} occur'' or ``The variable \textit{i} is always 3''.

\item Soundness: $ analysis(P) = true \implies S $

\item Completeness: $ S \implies analysis(P) = true $

\item Scalability: Time complexity.
\end{itemize}


\textbf{No analysis} can be sound and complete at the same time. If an
analysis is sound, then it is also incomplete, and vice versa.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Relation Theory}
This note originates from
Proofwiki\footnote{https://proofwiki.org}.

\subsection{Relation}

Let $S \times T$ be the Cartesian product of two sets $S$ and $T$.A
\textbf{relation} on $S \times T$ is an ordered triple
$ \mathcal{R} = (S, T, R) $ where $R \subseteq S \times T$ is a subset
of the Cartesian product of $S$ and $T$.

What this means is that a \textbf{relation} \textit{relates} (certain)
elements of one set or class $S$ with (certain) elements of another,
$T$. Not all elements of $S$ need to be related to every (or even any)
element of $T$.

\paragraph{Notation}

If $(x, y)$ is an ordered pair such that $(x, y) \in \mathcal{R}$, we
use the notation: $ s \mathcal{R} t$ or $ \mathcal{R}(s, t)$ and can
say:
\begin{itemize}
\item $s$ \textbf{bears} $\mathcal{R}$ to $t$
\item $s$ \textbf{stands in} the relation $\mathcal{R}$ to $t$
\end{itemize}

\paragraph{General Definition}

Let
$\mathbb{S} = \displaystyle \prod_{i=1}^n S_i = S_1 \times S_2 \times ... \times S_n $
be the Cartesian product on $n$ sets $S_1, ..., S_n$.

An \textbf{$n$-ary relation on} $\mathbb{S}$ is an ordered
$n+1$-tuple $\mathcal{R}$ defined as

\begin{math}
  \begin{array}{c}
    \\
    \mathcal{R} := (S_1, S_2, ..., S_n, R)\\
    \\
  \end{array}
\end{math}

where $\mathcal{R}$ is an arbitrary subset
$\mathcal{R} \subseteq \mathbb{S}$.

To indicate that $(s_1, s_2, ..., s_n) \in R$, we write:
$\mathcal{R}(s_1, s_2, ..., s_n)$


\paragraph{Unary Relation}

As a special case of an $n$-ary relation on $S$, note that when $n=1$
we define a \textbf{unary relation} on $S$ as
$\mathcal{R} \subseteq S$. That is, a \textbf{unary relation} is a
subset of $S$.



\subsection{Domain}
\label{sec:domain}

Let $\mathcal{R} \subseteq S \times T$ be a relation. The
\textbf{domain} of $\mathcal{R}$ is defined and denoted as:

\begin{math}
  \begin{array}{c}
    \\
    \mathtt{Dom}(\mathcal{R}) := \{ s \in S: \exists t \in T: (s, t) \in \mathcal{R} \}\\
    \\
  \end{array}
\end{math}

\paragraph{General Definition}

Let $\displaystyle \prod_{i=1}^n S_i$ be the Cartesian product of sets
$S_1$ to $S_n$. Let
$\mathcal{R} \subseteq \displaystyle \prod_{i=1}^n S_i$ be an $n$-ary
relation on $\displaystyle \prod_{i=1}^n S_i$.

The \textbf{domain of $\mathcal{R}$} is the set defined as:

\begin{math}
  \begin{array}{c}
    \\
    \mathtt{Dom}(\mathcal{R}) := \{ (s_1, s_2, ..., s_{n-1}) \in \displaystyle \prod_{i=1}^{n-1} S_i : \exists s_n \in S_n : (s_1, s_2, ..., s_n) \in \mathcal{R} \}\\
    \\
  \end{array}
\end{math}

The concept is usually encountered when $\mathcal{R}$ is an
endorelation on $S$:

\begin{math}
  \begin{array}{c}
    \\
    \mathtt{Dom}(\mathcal{R}) := \{ (s_1, s_2, ..., s_{n-1}) \in S^{n-1} : \exists s_n \in S : (s_1, s_2, ..., s_n) \in \mathcal{R} \}\\
    \\
  \end{array}
\end{math}


\subsection{Codomain}
\label{sec:codomain}

The \textbf{codomain} of a relation $\mathcal{R} \subseteq S \times T$
is the set $T$. It can be denoted as $\mathtt{Cdm}(\mathcal{R})$.


\subsection{Image}
\label{sec:image}

Let $\mathcal{R} \subseteq S \times T$ be a relation. The
\textbf{image} of $\mathcal{R}$ is the set:

\begin{math}
  \begin{array}{c}
    \\
    \mathtt{Img}(\mathcal{R}) := \mathcal{R}[S] = \{ t \in T: \exists s \in S: (s, t) \in \mathcal{R} \}\\
    \\
  \end{array}
\end{math}

\paragraph{General Definition}

Let $\displaystyle \prod_{i=1}^n S_i$ be the Cartesian product of sets
$S_1$ to $S_n$. Let
$\mathcal{R} \subseteq \displaystyle \prod_{i=1}^n S_i$ be an $n$-ary
relation on $\displaystyle \prod_{i=1}^n S_i$.

The \textbf{image of $\mathcal{R}$} is the set defined as:

\begin{math}
  \begin{array}{c}
    \\
    \mathtt{Img}(\mathcal{R}) := \{ s_n \in S_n: \exists (s_1, s_2, ..., s_{n-1} \in \displaystyle \prod_{i=1}^{n-1} S_i: (s_1, s_2, ..., s_n) \in \mathcal{R} \}\\
    \\
  \end{array}
\end{math}

The concept is usually encountered when $\mathcal{R}$ is an
endorelation on $S$:

\begin{math}
  \begin{array}{c}
    \\
    \mathtt{Img}(\mathcal{R}) := \{ s_n \in S: \exists (s_1, s_2, ..., s_{n-1}) \in S^{n-1}: (s_1, s_2, ..., s_n) \in \mathcal{R} \}\\
    \\
  \end{array}
\end{math}


\subsection{Endorelation}
\label{sec:endorelation}
Let $S \times S$ be the Cartesian product of a set or class $S$ with
itself. Let $\mathcal{R}$ be a \textbf{relation} on $S \times S$. Then
$\mathcal{R}$ is referred to as an \textbf{endorelation on} $S$.


The term \textbf{endorelation} is rarely seen. Once it is established
that the \textit{domain} and \textit{codomain} of a given relation are
the \textbf{same set}, further comment is rarely needed.


An \textbf{endorelation} is also called a \textbf{relation in} $S$, or
a \textbf{relation on} $S$. The latter term is discouraged, though,
because it can also mean a left-total relation, and confusion can
arise.

Some sources use the term \textbf{binary relation} exclusively to
refer to a \textbf{binary endorelation}.


\subsection{Many-to-One Relation}
\label{sec:many-to-one}

A relation $\mathcal{R} \subseteq S \times T$ is \textbf{many-to-one}
if and only if:

\begin{math}
  \begin{array}{c}
    \\
    \forall x \in \mathtt{Dom}(\mathcal{R}): \forall y_1, y_2 \in \mathtt{Cdm}(\mathcal{R}): (x, y_1) \in \mathcal{R} \land (x, y_2) \in \mathcal{R} \implies y_1 = y_2 \\
    \\
  \end{array}
\end{math}

That is, every element of the domain of $\mathcal{R}$ relates to no
more than one element of its codomain.

\subsection{One-to-Many Relation}
\label{sec:one-to-many}

A relation $\mathcal{R} \subseteq S \times T$ is \textbf{one-to-many}
if and only if:

\begin{math}
  \begin{array}{c}
    \\
    \forall y \in \mathtt{Img}(\mathcal{R}): (x_1, y) \in \mathcal{R} \land (x_2, y) \in \mathcal{R} \implies x_1 = x_2 \\
    \\
  \end{array}
\end{math}

That is, every element of the image of $\mathcal{R}$ relates to by
exactly one element of its domain. Note that the condition concerns
the elements in the \textit{image}, not the
\textit{codomain}\footnote{Thus, a one-to-many relation may leave some
  element(s) of the codomain unrelated}.

Also called as \textit{injective relation}\ref{sec:injection}.

\subsection{One-to-One Relation}
\label{sec:one-to-one}

A relation $\mathcal{R} \subseteq S \times T$ is \textbf{one-to-one}
if it is both many-to-one and one-to-many. That is, every element of
the domain of $\mathcal{R}$ relates to no more than one element of its
codomain, and every element of the image is related to by exactly one
element of its domain.


\subsection{Left-Total Relation}
\label{sec:left-total}

Let $S$ and $T$ be sets, and $\mathcal{R} \subseteq S \times T$ be a
relation in $S$ to $T$. Then $\mathcal{R}$ is \textbf{left-total} if
and only if:

\begin{math}
  \begin{array}{c}
    \\
    \forall s \in S: \exists t \in T: (s, t) \in \mathcal{R} \\
    \\
  \end{array}
\end{math}

That is, if and only if every element of $S$ relates to some element
of $T$, i.e., the domain of $\mathcal{R}$ equals to $S$.


\subsection{Right-Total Relation}
\label{sec:right-total}

Let $S$ and $T$ be sets, and $\mathcal{R} \subseteq S \times T$ be a
relation in $S$ to $T$. Then $\mathcal{R}$ is \textbf{right-total} if
and only if:

\begin{math}
  \begin{array}{c}
    \\
    \forall t \in T: \exists s \in S: (s, t) \in \mathcal{R} \\
    \\
  \end{array}
\end{math}

That is, if and only if every element of $T$ relates to by some
element of $S$, i.e., the image of $\mathcal{R}$ equals to $T$.


\subsection{Mapping}
\label{sec:mapping}

Let $S$ and $T$ be sets, and $S \times T$ be their Cartesian product.


\paragraph{Definition 1}

A \textbf{mapping} from $S$ to $T$ is a binary relation on
$S \times T$ which associates each element of $S$ with exactly one
element of $T$.

\paragraph{Definition 2}

A \textbf{mapping $f$ from $S$ to $T$}, denoted $f: S \to T$, is a
relation $f = (S, T, G)$ where $G \subseteq S \times T$ such that:

\begin{math}
  \begin{array}{l}
    \\
    \forall x \in S: \forall y_1, y_2 \in T: (x, y_1) \in G \land (x, y_2) \in G \implies y_1 = y_2 \\
    \text{and} \\
    \forall x \in S: \exists y in T: (x, y) \in G \\
    \\
  \end{array}
\end{math}

\paragraph{Definition 3}

A \textbf{mapping $f$ from $S$ to $T$}, denoted $f: S \to T$, is a
relation $f = (S, T, R)$ where $R \subseteq S \times T$ such that:

\begin{math}
  \begin{array}{l}
    \\
    \forall (x_1, y_1), (x_2, y_2) \in \mathcal{R}: y_1 \neq y_2 \implies x_1, x_2 \\
    \text{and} \\
    \forall x \in S: \exists y \in T: (x, y) \in \mathcal{R} \\
    \\
  \end{array}
\end{math}

\paragraph{Definition 4}

A \textbf{mapping from $S$ to $T$} is a relation on $S \times T$ which is:

\begin{itemize}
\item Many-to-one
\item Left-total, that is, defined for all elements in $S$
\end{itemize}

\paragraph{Self-Map}
\label{sec:self-map}

Let $S$ be a set. A \textbf{self-map on $S$} is a \textbf{mapping}
from $S$ to itself: $f: S \to S$.

\paragraph{Defined}
\label{sec:defined}

A mapping $f \subseteq S \times T$ is \textbf{defined} at $x \in S$ if
and only if:

\begin{math}
  \begin{array}{c}
    \\
    \exists y \in T: (x, y) \in f\\
    \\
  \end{array}
\end{math}

If for some $x \in S$, one has:

\begin{math}
  \begin{array}{c}
    \\
    \forall y \in T: (x, y) \notin f\\
    \\
  \end{array}
\end{math}

then $f$ is \textbf{not defined} or \textbf{undefined} at $x$, and
indeed, $f$ is not technically a mapping at all.


\subsection{Injection}
\label{sec:injection}

A mapping $f$ is an \textbf{injection} or \textbf{injective} if and
only if:

\begin{math}
  \begin{array}{c}
    \\
    \forall x_1, x_2 \in \mathtt{Dom}(f): f(x_1) = f(x_2) \implies x_1 = x_2 \\
    \\
  \end{array}
\end{math}

That is, an injection is a mapping such that the output
\textit{uniquely determines} its input.

\subsection{Surjection}
\label{sec:surjection}

Let $S$ and $T$ be sets and $f : S \to T$ be a mapping from $S$ to
$T$. $f$ is a \textbf{subjection} if and only if

\begin{math}
  \begin{array}{c}
    \\
    \forall y \in T: \exists x \in \mathtt{Dom}(f) : f(x) = y\\
    \\
  \end{array}
\end{math}

That is, if and only if $f$ is right-total.

Also called as \textbf{onto mapping}, or just \textbf{onto}\footnote{A
  mapping which is not surjective is hence described as \textbf{into}}.


\subsection{Bijection}
\label{sec:bijection}

A mapping $f: S \to T$ is a \textbf{bijection} if and only if both
\begin{itemize}
\item $f$ is an injection
\item $f$ is a surjection
\end{itemize}



\subsection{Symmetry}
\label{sec:symmetry}
The word \textit{symmetry} comes from Greek symmetria meaning
\textbf{measure together}.

\paragraph{Definition}

Let $\mathcal{R} \subseteq S \times S$ be a relation in $S$.

\subsubsection{Symmetric}

$\mathcal{R}$ is \textbf{symmetric} if and only if
$(x, y) \in \mathcal{R} \implies (y, x) \in \mathcal{R}$.

\subsubsection{Asymmetric}

$\mathcal{R}$ is \textbf{asymmetric} if and only if
$(x, y) \in \mathcal{R} \implies (y, x) \notin \mathcal{R}$.

\subsubsection{Antisymmetric}

$\mathcal{R}$ is \textbf{antisymmetric} if and only if
$ (x,y) \in \mathcal{R} \land (y, x) \in \mathcal{R} \implies x = y $.
That is, $\{(x, y), (y, x) \} \subseteq \mathcal{R} \implies x = y $.

\subsubsection{Non-symmetric}

$\mathcal{R}$ is \textbf{non-symmetric} if and only if it is neither
\textit{symmetric} nor \textit{asymmetric}.

\paragraph{Antisymmetric and Asymmetric}

Note the difference between:
\begin{itemize}
\item An \textit{asymmetric relation}, in which the fact that
  $(x, y) \in \mathcal{R}$ means that $(y, x)$ is defintely
  \textbf{not} in $\mathcal{R}$
\item An \textit{antisymmetric relation}, in which there \textit{may}
  be instances of both $(x, y) \in \mathcal{R}$ and
  $(y, x) \in \mathcal{R}$, but if there are, then it means that $x$
  and $y$ have to be the same object.
\end{itemize}


\subsection{Reflexivity}
\label{sec:reflexivity}

\paragraph{Definition}

Let $\mathcal{R} \subseteq S \times S$ be a relation in $S$.

\subsubsection{Reflexive}

$\mathcal{R}$ is \textbf{reflexive} if and only if
$ \forall x \in S : (x, x) \in \mathcal{R} $.

\subsubsection{Coreflexive}

$\mathcal{R}$ is \textbf{coreflexive} if and only if
$ \forall x, y \in S : (x, y) \in \mathcal{R} \implies x = y$.

\subsubsection{Antireflexive}

$\mathcal{R}$ is \textbf{antireflexive} if and only if
$ \forall x \in S: (x, x) \notin \mathcal{R}$.

\subsubsection{Non-reflexive}

$\mathcal{R}$ is \textbf{non-reflexive} if and only if it is neither
\textit{reflexive} nor \textit{antireflexive}.


\subsection{Transitivity}
\label{sec:transitivity}

\paragraph{Definition}

Let $\mathcal{R} \subseteq S \times S$ be a relation in $S$.

\subsubsection{Transitive}

$\mathcal{R}$ is \textbf{transitive} if and only if
$(x, y) \in \mathcal{R} \land (y, z) \in \mathcal{R} \implies (x, z)
\in \mathcal{R}$. That is,
$\{(x, y), (y, z)\} \subseteq \mathcal{R} \implies (x, z) \in
\mathcal{R}$.

\subsubsection{Antitransitive}

$\mathcal{R}$ is \textbf{antitransitive} if and only if
$ (x, y) \in \mathcal{R} \land (y, z) \in \mathcal{R} \implies (x, z)
\notin \mathcal{R}$. That is,
$ \{ (x, y), (y, z) \} \subseteq \mathcal{R} \implies (x, z) \notin
\mathcal{R} $.

\subsubsection{Non-transitive}

$\mathcal{R}$ is \textbf{non-transitive} if and only if it is neither
\textit{transitive} nor \textit{antitransitive}.


\subsection{Total Relation}
\label{sec:total-relation}

Let $\mathcal{R} \subseteq S \times S$ be a relation on a set
$S$. Then $\mathcal{R}$ is defined as \textbf{total} if and only if:

\begin{math}
  \begin{array}{c}
    \\
    \forall a, b \in S : (a, b) \in \mathcal{R} \lor (b, a) \in \mathcal{R} \\
    \\
  \end{array}
\end{math}

That is, if and only if every pair of elements is related.

Also called as \textbf{strictly connected}, or \textbf{complete
  relation}.

\subsection{Connected Relation}
\label{sec:connected-relation}

Let $\mathcal{R} \subseteq S \times S$ be a relation on a set
$S$. Then $\mathcal{R}$ is \textbf{connected} if and only if:

\begin{math}
  \begin{array}{c}
    \\
    \forall a, b \in S: a \neq b \implies (a, b) \in \mathcal{R} \lor (b,a) \in \mathcal{R} \\
    \\
  \end{array}
\end{math}


That is, if and only if every pair of \textit{distinct} elements is
related.


Also called as \textbf{weakly connected}, while \textit{strictly
  connected} refers to \textit{total
  relation}\ref{sec:total-relation}.


\subsection{Relation Compatible with Operation}
\label{sec:compatibility}

Let $(S, \circ)$ be a magma. Let $\mathcal{R}$ be a relation on $S$.

Then $\mathcal{R}$ is \textbf{compatible with $\circ$} if and only if:

\begin{itemize}
\item $\forall x, y, z \in S: x \mathcal{R} y \implies (x \circ z) \mathcal{R} (y \circ z)$
\item $\forall x, y, z \in S: x \mathcal{R} y \implies (z \circ x) \mathcal{R} (z \circ y)$
\end{itemize}


\subsection{Relational Structure}
\label{sec:relational-structure}

A \textbf{relational structure} is an ordered pair $(S, \mathcal{R})$,
where:

\begin{itemize}
\item $S$ is a set
\item $\mathcal{R}$ is an endorelation\ref{sec:endorelation} on $S$
\end{itemize}

Also called as a \textbf{relational system}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Binary Operation}
\label{sec:binary-operation}

A \textbf{binary operation} is a mapping $\circ$ from the Cartesian
product of two sets $S \times T$ to a universe $\mathbb{U}$:

\begin{math}
  \begin{array}{c}
    \\
    \circ: S \times T \to \mathbb{U} : \circ(s, t) = y \in \mathbb{U} \\
    \\
  \end{array}
\end{math}

If $S = T$ , then $\circ$ can be referred to as a \textbf{binary
  opeation on $S$}.

Some authors specify that a binary operation is defined such that
\textit{codomain} of $\circ$ is the same underlying set as that which
forms the \textit{domain}, that is: $\circ: S \times S \to S$.


\subsection{Algebraic Structure}
\label{sec:algebraic-structure}

An \textbf{algebraic structure} is an ordered tuple:

\begin{math}
  \begin{array}{c}
    (S, \circ_1, \circ_2, ..., \circ_n)
  \end{array}
\end{math}

where $S$ is a set which has one or more binary operations
$\circ_1, \circ_2, ..., \circ_n$ defined on all the elements of
$S \times S$.

An algebraic structure with one (binary) operation is thus an ordered
pair which can be denoted as $(S, \circ)$ or $(T, *)$ or
$\pair{S, \circ}$, and so on.

The set $S$ is called \textbf{underlying set}.


\subsection{Closure}
\label{sec:closure}

Let $(S, \circ)$ be an algebraic structure. Then $S$ has the property
of \textbf{closure under $\circ$} if and only if:

\begin{math}
  \begin{array}{c}
    \\
    \forall (x, y) \in S \times S : x \circ y \in S\\
    \\
  \end{array}
\end{math}


$S$ is said to be \textbf{closed under $\circ$}, or just that
$(S, \circ)$ \textbf{is closed}.

\subsection{Magma}
\label{sec:magma}

A \textbf{magma} is an algebraic structure $(S, \circ)$ such that $S$
is closed unser $\circ$.

That is, a magma is a pair $\pair{S, \circ}$ where:

\begin{itemize}
\item $S$ is a set
\item $\circ: S \times S \to S $ is a binary operation on $S$.
\end{itemize}




\subsection{Commutativity}
\label{sec:commutativity}

Let $(S, \circ)$ be an algebraic structure. Then $\circ$ is
\textbf{commutative on $S$} if and only if:

\begin{math}
  \begin{array}{c}
    \\
    \forall x, y \in S: x \circ y = y \circ x\\
    \\
  \end{array}
\end{math}

\subsection{Absorption Law}
\label{sec:absorption}

Let $(S, \circ, *)$ be an algebraic structure. Let both $\circ$ and
$*$ be commutative.

Then $\circ$ \textbf{absorbs} $*$ if and only if:

\begin{math}
  \begin{array}{c}
    \\
    \forall a, b \in S: a \circ (a * b) = a\\
    \\
  \end{array}
\end{math}

This equality is called the \textbf{absorption law of $\circ$ for
  $*$}.



\subsection{Associativity}
\label{sec:associativity}

Let $(S, \circ)$ be an algebraic structure. Then $\circ$ is
\textbf{associative on $S$} if and only if:

\begin{math}
  \begin{array}{c}
    \\
    \forall x, y, z \in S: (x \circ y) \circ z = x \circ (y \circ z)\\
    \\
  \end{array}
\end{math}


\subsection{Idempotence}
\label{sec:idempotence}

\paragraph{Idempotent Element}
Let $(S, \circ)$ be a magma. Let $x \in S$ have the property that
$ x \circ x = x$.

Then $x \in S $ is described as \textbf{idempotent (element) under the
  operation $\circ$}.

\paragraph{Idempotent Operation}
Let $(S, \circ)$ be a magma.


If \textit{all} the elements of $S$ are \textit{idempotent} under
$\circ$, then the term can be applied to the operation itself. Thus, a
binary operation $\circ$ is \textbf{idempotent} if and only if:

\begin{math}
  \begin{array}{c}
    \\
    \forall x \in S: x \circ x = x\\
    \\
  \end{array}
\end{math}

Examples are: set union, set intersection, ...



\subsection{Identity}
\label{sec:identity}

Let $(S, \circ)$ be an algebraic structure.


\paragraph{Left Identity}
An element $e_L \in S$ is called a \textbf{left identity} if and only
if:

\begin{math}
  \begin{array}{c}
    \\
    \forall x \in S: e_L \circ x = x\\
    \\
  \end{array}
\end{math}

\paragraph{Right Identity}
An element $e_R \in S$ is called a \textbf{right identity} if and only
if:

\begin{math}
  \begin{array}{c}
    \\
    \forall x \in S: x \circ e_R = x\\
    \\
  \end{array}
\end{math}


\paragraph{Identity}
An element $e \in S$ is called an \textbf{identity (element)} if and
only if it is both a \textit{left identity} and \textit{right
  identity}

\begin{math}
  \begin{array}{c}
    \\
    \forall x \in S: e \circ x = x \circ e =  x\\
    \\
  \end{array}
\end{math}


\paragraph{Uniqueness of Identity}

Suppose $e_1$ and $e_2$ are both identity elements of $(S, \circ)$.

Then by the definition of identity elements:

\begin{math}
  \begin{array}{c}
    \\
    \forall s \in S: s \circ e_1 = s = e_2 \circ s\\
    \\
  \end{array}
\end{math}

Then:

\begin{math}
  \begin{array}{c}
    e_1 = e_2 \circ e_1 = e_2 \\
    \therefore e_1 = e_2
  \end{array}
\end{math}



\subsection{Semigroup}
\label{sec:semigroup}

Let $(S, \circ)$ be a magma. Then this magma is a \textbf{semigroup}
if and only if $\circ$ is \textit{associative} on $S$.


That is, a \textbf{semigroup} is an algebraic structure which is
\textit{closed} and whos operation is \textit{associative}.


\subsection{Monoid}
\label{sec:monoid}

A \textbf{monoid} is a \textit{semigroup} with an \textit{identity
  element}.

\paragraph{Monoid is not Empty}

Let $(S, \circ)$ be a monoid. By definition:

\begin{math}
  \begin{array}{c}
    \text{Identity: } \exists e_S \in S : \forall a \in S: a \circ e_S = a = e_S \circ a
  \end{array}
\end{math}

So a monoid must \textit{at least} have an identity.

Therefore $e_S \in S $ and so $S$ is not the empty set.


\subsection{Inverse}
\label{sec:inverse}

Let $(S, \circ)$ be a monoid whose identity is $e_S$.

\paragraph{Left Inverse}
An element $x_L \in S$ is called a \textbf{left inverse} of $x$ if and
only if:

\begin{math}
  \begin{array}{c}
    x_L \circ x = e_S
  \end{array}
\end{math}


\paragraph{Right Inverse}
An element $x_R \in S$ is called a \textbf{right inverse} of $x$ if and
only if:

\begin{math}
  \begin{array}{c}
    x \circ x_R = e_S
  \end{array}
\end{math}

\paragraph{Inverse}

Let $x , y \in S $ be elements. The element $y$ is an \textbf{inverse
  of $x$} if and only if $y$ is both a \textit{left inverse} and
\textit{right inverse}.


\subsection{Group}
\label{sec:group}

A \textbf{group} is a \textit{semigroup} with an \textit{identity}
(that is, a \textit{monoid}) in which every element has an
\textit{inverse}.

\begin{itemize}
\item G0: Closure. $ \forall a, b \in G: a \circ b \in G$
\item G1: Associativity. $ \forall a, b, c \in G: a \circ (b \circ c) = (a \circ b) \circ c $
\item G2: Identity. $\exists e in G : \forall a in G: e \circ a = a = a \circ e $
\item G3: Inverse. $\forall a in G: \exists b in G: a \circ b = e = b \circ a$
\end{itemize}


\subsection{Set Equivalence}
\label{sec:set-eq}

Let $S$ and $T$ be sets. Then $S$ and $T$ are \textbf{equivalent} if
and only if there exists a \textbf{bijection} $f : S \to T$ between
the elements of $S$ and those of $T$. That is, if they have the
\textbf{same cardinality}. This can be written as $S \sim T$.


\subsection{Finite Set}
\label{sec:finite-set}

A set $S$ is defined as \textbf{finite} if and only if

\begin{math}
  \begin{array}{c}
    \\
    \exists n \in \mathbb{N}: S \sim \mathbb{N}_{< n}\\
    \\
  \end{array}
\end{math}

where $\sim$ denotes \textit{set equivalence}.

That is, if there exists an element $n$ of the set of natural numbers
$\mathbb{N}$ such that the set of all elements of $\mathbb{N}$ less
than $n$ is equivalent to $S$.

Equivalently, a finite set is a set with a count.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ordering}
\label{sec:ordering}

\paragraph{Definition}

Let $S$ be a set. An \textbf{ordering on} $S$ is a relation
$\mathcal{R}$ on $S$ such that:

\begin{itemize}
\item $\mathcal{R}$ is \textbf{reflexive}\ref{sec:reflexivity}, i.e.,
  $\forall a \in S: a \mathcal{R} a$.
\item $\mathcal{R}$ is \textbf{transitive}\ref{sec:transitivity},
  i.e.,
  $\forall a, b, c \in S: a \mathcal{R} b \land b \mathcal{R} c
  \implies a \mathcal{R} c$.
\item $\mathcal{R}$ is \textbf{antisymmetric}\ref{sec:symmetry}, i.e.,
  $\forall a \in S : a \mathcal{R} b \land b \mathcal{R} a \implies a
  = b$.
\end{itemize}

It is not demanded for an ordering $\preceq$, defined in its most
general form on a set $S$, that \textit{every} pair of elements of $S$
is related by $\preceq$. They may be, or they may not be, depending on
the specific nature of both $S$ and $\preceq$.

If it \textit{is} the case that $\preceq$ is a \textbf{connected
  relation}, that is, that every pair of distinct elements is related
by $\preceq$, then $\preceq$ is called a \textbf{total ordering}.

If it is \textit{not} the case that $\preceq$ is connected, then
$\preceq$ is called a \textbf{partial ordering}.


\paragraph{Notation}

Symbols used to denote a general \textbf{ordering relation} are
usually variants on $\preceq$, $\leq$, and so on.

Thus, $a \preceq b$ can be read as:
\begin{itemize}
\item $a$ \textbf{precedes, or is the same as} $b$.
\item $b$ \textbf{succeeds, or is the same as} $a$.
\end{itemize}

\paragraph{Smaller and Larger}

An \textbf{ordering} can often be considered to be a comparison of the
\textbf{size} of objects, perhaps in some intuitive sense. This is
particularly applicable in the context of numbers. Thus the expression
$A \preceq B$ can in such contexts be interpreted as:
\begin{itemize}
\item $A$ is \textbf{smaller than} $B$
\item $A$ is \textbf{less than} $B$
\item $B$ is \textbf{larger than} $A$
\item $B$ is \textbf{greater than} $A$
\end{itemize}

In natural language, such terms are called \textbf{comparative
  adjectives}, or just \textbf{comparatives}.

Depending on the nature of the set being ordered, and depending on the
nature of the ordering relation, this interpretation of an ordering as
a comparison of size may not be intellectually sustainable.

\subsection{Ordered Set}
\label{sec:ordered-set}

An \textbf{ordered set} is a relational structure $(S, \preceq)$ such
that the relation $\preceq$ is an ordering.


\subsection{Ordered Structure}
\label{sec:ordered-structure}

An \textbf{ordered structure} $(S, \circ, \preceq)$ is a set $S$ such
that:

\begin{itemize}
\item $(S, \circ)$ is an algebraic structure
\item $(S, \preceq)$ is an ordered set
\item $\preceq$ is \textit{compatible}\ref{sec:compatibility} with
  $\circ$
\end{itemize}


\subsection{Monotone}

\paragraph{Increasing}
\label{sec:increasing}

Let $(S, \preceq_1)$ and $(T, \preceq_2)$ be ordered sets. Let
$\phi: S \to T$ be a mapping. Then $phi$ is \textbf{increasing} if and
only if:

\begin{math}
  \begin{array}{c}
    \\
    \forall x, y \in S: x \preceq_1 y \implies \phi(x) \preceq_2 \phi(y) \\
    \\
  \end{array}
\end{math}

Note that this definition also holds if $S = T$.

It is also called as \textbf{order-preserving}, \textbf{isotone}, and
\textbf{non-decreasing}.

\paragraph{Decreasing}
\label{sec:decreasing}

Let $(S, \preceq_1)$ and $(T, \preceq_2)$ be ordered sets. Let
$\phi: S \to T$ be a mapping. Then $phi$ is \textbf{decreasing} if and
only if:

\begin{math}
  \begin{array}{c}
    \\
    \forall x, y \in S: x \preceq_1 y \implies \phi(y) \preceq_2 \phi(x) \\
    \\
  \end{array}
\end{math}

Note that this definition also holds if $S = T$.

It is also called as \textbf{order-inverting},
\textbf{order-reversing}, \textbf{anitone}, and
\textbf{non-increasing}.


\paragraph{Monotone}
\label{sec:monotone}


Let $(S, \preceq_1)$ and $(T, \preceq_2)$ be ordered sets. Let
$\phi: S \to T$ be a mapping. Then $phi$ is \textbf{monotone} if and
only if it is either \textit{increasing} or \textit{decreasing}.

Note that this definition also holds if $S = T$.




\subsection{Upper Bound}
\label{sec:upper-bound}

Let $(S, \preceq)$ be an ordered set. Let $T \subseteq S$. An
\textbf{upper bound for $T$ in $S$} is an element $m \in S$ such that:

\begin{math}
  \begin{array}{c}
    \\
    \forall t \in T : t \preceq m\\
    \\
  \end{array}
\end{math}

That is, $m$ \textit{succeeds} every elements of $T$.



\subsection{Supremum}
\label{sec:supremum}

Let $(S, \preceq)$ be an ordered set. Let $T \subseteq S$. An element
$c \in S$ is the \textbf{supremum of $T$ in $S$} if and only if:

\begin{itemize}
\item $c$ is an \textit{upper bound} of $T$ in $S$
\item $c \preceq d$ for all upper bounds $d$ of $T$ in $S$
\end{itemize}

If there exists a \textbf{supremum} of $T$, we say that:

\begin{itemize}
\item $T$ admits a supremum (in $S$)
\item $T$ has a supremum (in $S$)
\end{itemize}


Particularly in the field of analysis, the supremum of a set $T$ is
often referred to as the \textbf{least upper bound of $T$} and denoted
$\mathtt{lub}(T)$.


\paragraph{Uniqueness of Supremum}

Let $c$ and $c'$ both be suprema of $T$ in $S$. From the definition of
supremum, $c$ and $c'$ are upper bounds of $T$ in $S$.

By that definition:

\begin{itemize}
\item $c$ is an upper bound of $T$ in $S$, and $c'$ is a supremum of
  $T$ in $S$ implies that $c' \preceq c$.
\item $c'$ is an upper bound of $T$ in $S$, and $c$ is supremum of $T$
  in $S$ implies that $c preceq c'$.
\end{itemize}

Therefore,

\begin{math}
  \begin{array}{ll}
    \\
    c' \preceq c \land c \preceq c'\\
    \therefore c = c' & \because \preceq \text{ is antisymmetry}\\
    \\
  \end{array}
\end{math}

Q.E.D.


\subsection{Join}
\label{sec:join}

Let $(S, \preceq)$ be an ordered set. Let $a, b \in S$. Let their
supremum $\mathtt{sup} \{a, b\}$ exist in $S$.

Then the \textbf{join of $a$ and $b$}\footnote{some sources refer to
  this as the \textbf{union} of $a$ and $b$} is defined as:

\begin{math}
  \begin{array}{c}
    \\
    a \vee b = \mathtt{sup} \{ a, b \}\\
    \\
  \end{array}
\end{math}

Expanding the definition of supremum, one sees that $c = a \vee b$ if
and only if:

\begin{math}
  \begin{array}{c}
    \\
    a \preceq c\text{ and }b \preceq c\text{ and }\forall s \in S: a \preceq s \land b \preceq s \implies c \preceq s\\
    \\
  \end{array}
\end{math}



\subsection{Lower Bound}
\label{sec:lower-bound}

Let $(S, \preceq)$ be an ordered set. Let $T \subseteq S$. A
\textbf{lower bound for $T$ in $S$} is an element $m \in S$ such that:

\begin{math}
  \begin{array}{c}
    \\
    \forall t \in T: m \preceq t\\
    \\
  \end{array}
\end{math}

That is, $m$ \textit{precedes} every elements of $T$.


\subsection{Infimum}
\label{sec:infimum}

Let $(S, \preceq)$ be an ordered set. Let $T \subseteq S$. An element
$c \in S$ is the \textbf{infimum of $T$ in $S$} if and only if:

\begin{itemize}
\item $c$ is a \textit{lower bound} of $T$ in $S$
\item $d \preceq c$ for all lower bounds $d$ of $T$ in $S$
\end{itemize}

If there exists an \textbf{infimum} of $T$, we say that:

\begin{itemize}
\item $T$ admits an infimum (in $S$)
\item $T$ has an infimum (in $S$)
\end{itemize}


Particularly in the field of analysis, the infimum of a set $T$ is
often referred to as the \textbf{greatest lower bound of $T$} and
denoted as $\mathtt{glb}(T)$.


\subsection{Meet}
\label{sec:meet}

Let $(S, \preceq)$ be an ordered set. Let $a, b \in S$. Let their
infimum $\mathtt{inf} \{a, b\}$ exist in $S$.

Then the \textbf{meet of $a$ and $b$}\footnote{some sources refer to
  this as the \textbf{intersection} of $a$ and $b$} is defined as:

\begin{math}
  \begin{array}{c}
    \\
    a \wedge b =\mathtt{inf} \{ a, b \}\\
    \\
  \end{array}
\end{math}


Expanding the definition of infimum, one sees that $c = a \wedge b$ if
and only if:

\begin{math}
  \begin{array}{c}
    \\
    c \preceq c\text{ and }c \preceq b\text{ and } \forall s \in S: s \preceq a \land s \preceq b \implies s \preceq c\\
    \\
  \end{array}
\end{math}


\subsection{Semilattice}
\label{sec:semilattice}

A semigroup $(S, \circ)$ is called a \textbf{semilattice} if and only
if $\circ$ is a \textit{commutative} and \textit{idempotent}
operation.

Thus, an algebraic structure is a \textbf{semilattice} if and only if
it satisfies the semilattice axioms:

\begin{itemize}
\item SL0: Closure for $\circ$. $\forall a, b \in S: a \circ b \in S$
\item SL1: Associativity of $\circ$.
  $\forall a, b, c \in S: (a \circ b) \circ c = a \circ (b \circ c)$
\item SL2: Commutativity of $\circ$.
  $\forall a, b \in S: a \circ b = b \circ a$
\item SL3: Idempotence of $\circ$. $\forall a \in S: a \circ a = a$
\end{itemize}


\subsubsection{Join Semilattice}

Let $(S, \vee, \preceq)$ be an ordered structure and $\vee$ is join
operator. Suppose that $\forall a, b \in S: a \vee b \in S$ where
$a \vee b$ is the join of $a$ and $b$ with respect to $\preceq$.

Then the ordered structure $(S, \vee, \preceq)$ is called a
\textbf{join semilattice}.


\subsubsection{Meet Semilattice}

Let $(S, \wedge, \preceq)$ be an ordered structure and $\wedge$ is
meet operator. Suppose that $\forall a, b \in S: a \wedge b \in S$
where $a \wedge b$ is the meet of $a$ and $b$ with respect to
$\preceq$.

Then the ordered structure $(S, \wedge, \preceq)$ is called a
\textbf{meet semilattice}.


\paragraph{Semilattice Induces Ordering}
\label{sec:semilattice-induces-ordering}

Let $(S, \circ)$ be a semlilattice. Let $\preceq$ be the relation on
$S$ defined by $\forall a, b \in S: a \preceq b \iff a \circ b = b$.

Then $\preceq$ is an ordering.

\subparagraph{Proof}

Let's verify that $\preceq$ satisfies the three conditions for an
ordering.

\subparagraph{Reflexivity}

Since $\circ$ is \textit{idempotent},
$\forall a \in S: a \circ a = a$. Hence $a \preceq a$. Thus $\preceq$
is \textit{reflexive}.

\subparagraph{Antisymmetry}

Suppose that $a \preceq b$ and $b \preceq a$. Then from the first
relation: $a \circ b = b$ and from the second: $b \circ a =a$.

Since $\circ$ is \textit{commutative}, it follows that $a = b$. Hence
$\preceq$ is \textit{antisymmetric}.

\subparagraph{Transitivity}

Suppose that $a \preceq b$ and $b \preceq c$.

Then:

\begin{math}
  \begin{array}{lcll}
    a \circ c & = & a \circ (b \circ c) & \because b \preceq c \\
              & = & (a \circ b) \circ c & \because \circ \text{ is associative} \\
              & = & b \circ c & \because a \preceq b \\
              & = & c & \because b \preceq c \\
    \therefore a \preceq c
  \end{array}
\end{math}

Thus, $\preceq$ is \textit{transitive}.


\subsection{Lattice}
\label{sec:lattice}

\paragraph{Definition 1}

Let $(S, \preceq)$ be an ordered set. Suppose that $S$ admits all
finite non-empty suprema and finite non-empty infima. Denote with
$\wedge$ and $\vee$ the join and meet operations on $S$, respectively.

Then the ordered structure $(S, \wedge, \vee, \preceq)$ is called a
\textbf{lattice}.

\paragraph{Definition 2}

Let $(S, \wedge, \vee, \preceq)$ be an ordered structure.

Then $(S, \wedge, \vee, \preceq)$ is called a \textbf{lattice} if and
only if:

\begin{itemize}
\item $(S,\vee, \preceq)$ is a join semilattice
\item $(S,\wedge, \preceq)$ is a meet semilattice
\end{itemize}

\paragraph{Definition 3}

Let $(S, \vee)$ and $(S, \wedge)$ be semilattices on a set $S$.

Suppose that $\vee$ and $\wedge$ satisfy the \textit{absorption laws}, that is,

\begin{itemize}
\item $\forall a, b \in S: a \vee (a \wedge b) = a$
\item $\forall a, b \in S: a \wedge (a \vee b) = a$
\end{itemize}

Let $\preceq$ be the ordering on $S$ defined by:

\begin{math}
  \begin{array}{c}
    \forall a, b \in S: a \preceq b \iff a \vee b = b
  \end{array}
\end{math}

as on Semilattice Induces
Ordering\ref{sec:semilattice-induces-ordering}.

Then the ordered structure $(S, \vee, \wedge, \preceq)$ is called a
\textbf{lattice}.


\subsection{Complete Lattice}
\label{sec:complete-lattice}

Let $(S, \preceq)$ be a lattice. Then, $(S, \preceq)$ is a
\textbf{complete lattice} if and only if:

\begin{math}
  \begin{array}{ll}
    \\
    & \forall T \subseteq S: T \text{ admits both a \textit{supremum} and an \textit{infimum}} \\
    \text{or} & \forall T \subseteq S: \mathtt{inf}T, \mathtt{sup}T \in S \\
    \\
  \end{array}
\end{math}

That is, if and only if all subsets $T$ of $S$ have both a supremum
and an infimum.

Also called as \textbf{complete ordered set}


\subsection{Pre-Ordering}
\label{sec:pre-ordering}

Let $\mathcal{R} \subseteq S \times S$ be a relation on a set
$S$. $\mathcal{R}$ is a \textbf{pre-ordering} if and only if:

\begin{itemize}
\item \textbf{Reflexive}: $\forall a \in S: a \mathcal{R} a $
\item \textbf{Transitive}:
  $\forall a,b,c \in S: a \mathcal{R} b \land b \mathcal{R} c \implies
  a \mathcal{R} c $
\end{itemize}

\paragraph{Pre-Ordered Set}

Let $S$ be a set, and $\precsim$ be a \textbf{preordering} on
$S$. Then the relational structure $(S, \precsim)$ is called a
\textbf{preordered set}.


\paragraph{Partial vs. Total}

Note that this definition of preordering does not demand that
\textit{every} pair of elements of $S$ is related by $\precsim$. The
way we have defined a preordering, they may be, or they may not be,
depending on the context.

If it \textit{is} the case that $\precsim$ is a \textit{connected
  relation}\ref{sec:connected-relation}, i.e., that \textit{every
  pair} of elements is related by $\precsim$, then $\precsim$ is
called a total preordering.

If it is \textit{not} the case that $\precsim$ is connected, then
$\precsim$ is called a partial preordering.


\subsection{Directed Set}
\label{sec:directed-set}

Let $(S, \precsim)$ be a preordered set. Then $(S, \precsim)$ is a
\textbf{directed set} if and only if every pair of elements of $S$ has
an \textbf{upper bound}\ref{sec:upper-bound} in $S$:

\begin{math}
  \begin{array}{c}
    \\
    \forall x, y \in S: \exists m \in S: x \precsim m \land y \precsim m \\
    \\
  \end{array}
\end{math}

Also called as a \textbf{directed preorder}, \textbf{filtered set}, or
\textbf{upward directed set}.

\paragraph{Directed Subset}
\label{sec:directed-subset}

Let $(S, \precsim)$ be a preordered set. Let $H$ be a non-empty subset
of $S$. Then $H$ is a \textbf{directed subset} of $S$ if and only if:

\begin{math}
  \begin{array}{c}
    \\
    \forall x, y \in H: \exists m \in H: x \precsim m \land y \precsim m\\
    \\
  \end{array}
\end{math}



\subsection{Well-Ordering}
\label{sec:well-ordering}

Let $(S, \preceq)$ be an ordered set.

The ordering $\preceq$ is a \textbf{well-ordering} on $S$ if and only
if \textbf{every} non-empty subset of $S$ has a smallest element under
$\preceq$. That is,


\begin{math}
  \begin{array}{c}
    \\
    \forall T \subseteq S: \exists a \in T : \forall x \in T : a \preceq x
    \\
  \end{array}
\end{math}

\subsection{Strict Ordering}
\label{sec:strict-ordering}

Let $\mathcal{R}$ be a relation on a set $S$. Then $\mathcal{R}$ is a
\textbf{strict ordering} on $S$ if and only if:

\subsubsection{Definition 1}

\begin{itemize}
\item \textbf{Asymmetry}:
  $\forall a, b \in S : a \mathcal{R} b \implies \neg b \mathcal{R} a$
\item \textbf{Transitivity}:
  $\forall a, b, c \in S: a \mathcal{R} b \land b \mathcal{R} c
  \implies a \mathcal{R} c$
\end{itemize}

\subsubsection{Definition 2}

\begin{itemize}
\item \textbf{Antireflexivity}:
  $\forall a \in S: \neg (a \mathcal{R} a) $
\item \textbf{Transitivity}:
  $\forall a, b, c \in S: a \mathcal{R} b \land b \mathcal{R} c
  \implies a \mathcal{R} c$
\end{itemize}

Symbols used to denote a general strict ordering are usually variants
on $\prec$, $<$, and so on.


\subsection{Total Ordering}
\label{sec:total-ordering}

Let $\mathcal{R} \subseteq S \times S$ be a relation on a set $S$.

$\mathcal{R}$ is a \textbf{total ordering} on $S$ if and only if:

\begin{itemize}
\item $\mathcal{R}$ is an ordering on $S$
\item $\mathcal{R}$ is connected\ref{sec:connected-relation}
\end{itemize}

That is, $\mathcal{R}$ is an ordering with no non-comparable pairs:


\begin{math}
  \begin{array}{c}
    \\
    \forall x, y \in S: x \mathcal{R} y \lor y \mathcal{R} x\\
    \\
  \end{array}
\end{math}

Also called as a \textbf{linear ordering}, or a \textbf{simple
  ordering}.

If it is necessary to emphasises that a total ordering is \textbf{not}
strict, then the term \textbf{weak total ordering} may be used.

\subsubsection{Chain}
\label{sec:chain}

Let $(S, \preceq)$ be an ordered set. A \textbf{chain in $S$} is a
totally ordered subset of $S$. Thus, a totally ordered set is itself a
chain in its own right.


\subsection{Partial Ordering}
\label{sec:partial-ordering}

Let $(S, \preceq)$ be an ordered set. Then the ordering $\preceq$ is a
\textbf{partial ordering} on $S$ if and only if $\preceq$ is
\textbf{not connected}\ref{sec:connected-relation}.

That is, if and only if $(S, \preceq)$ has at least one pair which is
non-comparable:

\begin{math}
  \begin{array}{c}
    \\
    \exists x, y \in S : x \npreceq y \land y \npreceq x \\
    \\
  \end{array}
\end{math}


It it is necessary to emphasizes that a partial ordering is
\textbf{not strict}\ref{sec:strict-ordering}, then the term
\textbf{weak partial ordering} may be used.


\subsection{Partially Ordered Set}
\label{sec:poset}

A \textbf{partially ordered set} is a relational structure
$(S, \preceq)$ such that $\preceq$ is a partial ordering.

The partially ordered set $(S, \preceq)$ is said to be
\textbf{partially ordered by $\preceq$}.



\subsection{Complete Partial Ordering}
\label{sec:cpo}

A \textbf{complete partial order} abbreviated \textbf{cpo} can,
depending on context, refer to any of the following concepts:

\begin{itemize}
\item A partially ordered set is a \textbf{directed-complete partial
    order (dcpo)} if each of its \textit{directed
    subsets}\ref{sec:directed-subset} has a supremum.
\item A partially ordered set is a \textbf{point directed-complete
    partial order} if it is a dcpo with a \textit{least element}.
\item A partially ordered set is a \textbf{$\omega$-complete partial
    order ($\omega$-cpo)} if it is a poset in which every
  $\omega$-chain $(x_1 \leq x_2 \leq x_3 \leq ...)$ has a supremum
  that belongs to the underlying set of the poset. Every dcpo is an
  $\omega$-cop, since every $\omega$-chain is a directed set, but the
  converse is not true.
\end{itemize}


\paragraph{Properties}

An ordered set $P$ is a pointed dcpo if and only if every chain has a
supremum in $P$, i.e., $P$ is chain-complete. Alternatively, an
ordered set $P$ is an pointed dcpo if and only if every
order-preserving self-map of $P$ has a least fixpoint. Every set $S$
can be turned into a pointed dcpo by adding a least element $\bot$ and
introducing a flat order with $\bot \leq s$ and $s \leq s$ for every
$s \in S$ and no other order relations.


\subsubsection{(Scott) Continuous}
\label{sec:scott-continuous}

A function $f$ between two dcpos $P$ and $Q$ is called \textbf{(Scott)
  continuous} if it maps directed sets to directed sets while
preserving their suprema:

\begin{itemize}
\item $f(D) \subseteq Q$ is directed for every directed $D \subseteq P$
\item $f(\mathtt{sup}D) = \mathtt{sup} f(D)$ for every directed $D subseteq P$
\end{itemize}

Note that every continuous function between dcpos is a
\textit{monotone function}\ref{sec:monotone}.


\subsubsection{Kleene Fixpoint Theorem}
\label{sec:fixpoint-theorem}

Suppose $(L, \sle)$ is directed-complete partial order with a least
element, and $f: L \to L$ be a Scott-continuous (and therefore
monotone) function. Then $f$ has a least fixed point, which is the
supremum of the ascending Kleene chain of $f$.

The \textbf{ascending Kleene chain} of $f$ is the chain

\begin{math}
  \begin{array}{c}
    \\
    \bot \sle f(\bot) \sle f(f(\bot)) \sle ... \sle f^n(\bot) \sle ...\\
    \\
  \end{array}
\end{math}

obtained by iterating $f$ on the least element $\bot$ of $L$. The
theorem states that

\begin{math}
  \begin{array}{c}
    \\
    \mathtt{lfp}(f) = \mathtt{sup}( \{ f^n(\bot) | n \in \mathbb{N}\})\\
    \\
  \end{array}
\end{math}

where $\mathtt{lfp}$ denotes the least fixed point.


Every \textit{order-preserving}\ref{sec:monotone}
\textit{self-map}\ref{sec:self-map} $f$ of a cpo $(P, \bot)$ has a
least fixpoint. If $f$ is continuous, then this fixpoint is equal to
the supremum of the iterates $(\bot, f(\bot), f(f(\bot)), ...)$ of
$\bot$.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Soundness and Completeness}

\subsection{Soundness}


It is called \textbf{sound} if an analysis for program \textsl{P} says
that it satisfies property \textsl{S}, then the program will truly
satisfy that property.

\textbf{Sound} but \textbf{incomplete} analysis have \textbf{false
  positive}. In other words, it does \textbf{not prove} programs that
satisfy the property.

\begin{figure}[h]
  \includegraphics[width=\textwidth]{sound}
  \caption{Sound Analysis}
  \label{fig:sound}
\end{figure}

Figure \ref{fig:sound} shows a sound analysis (the blue area). It
proves correctly for the programs $ p_1 $ and $ p_2 $ that satisfy the
specification ($S(P)$). However, since this analysis is
\textit{incomplete}, it has \textbf{false positives}: it \textbf{does
  not prove} for programs $ p_3 $ and $ p_4 $ that satisfy $S$. In
other words, it proves that $ p_3 $ and $ p_4 $ satisfy $ \neg S(P) $
by emiting \textbf{alarms}, even though they are actually satisfy
$ S(P) $. False positives are also called as \textbf{false alarms}.

\subsection{Completeness}

An analysis is called \textbf{complete} when a program satisfies a
property \textsl{S}, the analysis for that program says that it will
satisfy that property.


\textbf{Complete} but \textbf{unsound} analysis have \textbf{false
  negative}. In other words, it \textbf{wrongly proves} programs that
does not satisfy the property.


\begin{figure}[h]
  \includegraphics[width=\textwidth]{complete}
  \caption{Complete Analysis}
  \label{fig:complete}
\end{figure}

Figure \ref{fig:complete} shows a complete analysis (also the blue
area). It proves all the programs. In other words, it \textbf{wrongly
  proves} for programs $ p_5 $ and $ p_6 $ that actually do not
satisfy $ S(P) $ by accepting those programs (i.e., emitting no
alarms), thus it is \textit{unsound}. Those programs are \textbf{false
  negatives}.


The table \ref{tab:summary} shows the summary of soundness and
completeness.

\begin{table}[ht]
  \centering
  \caption{Sound \& Complete Summary}
  \label{tab:summary}

  \begin{tabular}[t]{l>{\raggedright}p{0.3\linewidth}>{\raggedright\arraybackslash}p{0.3\linewidth}}
    \hline
    & $ S(P) $ & $ \neg S(P) $ \\
    \hline
    Prove \texttt{(accept)} & \textbf{True negative} (\textsl{correct inference}) & False negative \\
    Not prove \texttt{(reject, alarm)} & False positive & \textbf{True positive} (\textsl{correct inference}) \\
    \hline
  \end{tabular}
\end{table}%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Semantics of Program}
\label{sec:semantics}

Semantics of the program defines the \textbf{meaning} of the program
that is grammatically correct.

\subsection{Prerequisites}

\subsubsection{Simple Language Syntax}

\begin{math}
  \begin{array}{llll}
    a \to & \mid n & \mid x & \mid a_1 + a_2 \\
          & \mid a_1 \times a_2 & \mid a_1 - a_2 \\
    b \to & \mid \mathtt{true} & \mid \mathtt{false} & \mid a_1 = a_2  \\
          & \mid a_1 \leq a_2 & \mid \neg b & \mid b_1 \land b_2 \\
    c \to & \mid x := a  & \mid \mathtt{skip} & \mid c_1 ; c_2 \\
          & \mid \mathtt{if} \text{ }b \text{ }c_1\text{ }c_2 & \mid \mathtt{while} \text{ }b \text{ }c \\
  \end{array}
\end{math}

A state $ s \in State $ is a function from variables to values:
\begin{math}
  \begin{array}{rcl}
    State & : & Var \to \mathbb{Z} \\
  \end{array}
\end{math}


\subsubsection{Semantics of Arithmetic Expressions}

\begin{math}
  \begin{array}{rcl}
    \mathcal{A} & : & \mathtt{Aexp} \to State \to \mathbb{Z} \\
    \mathcal{A} \SEM{ a } & : & State \to \mathbb{Z} \\
    \mathcal{A} \SEM{ n } (s) & = & n \\
    \mathcal{A} \SEM{ x } (s) & = & s(x) \\
    \mathcal{A} \SEM{ a_1 + a_2 } (s) & = & \mathcal{A} \SEM{ a_1 } (s) + \mathcal{A} \SEM{ a_2 } (s) \\
    \mathcal{A} \SEM{ a_1 \times a_2 } (s) & = & \mathcal{A} \SEM{ a_1 } (s) \times \mathcal{A} \SEM{ a_2 } (s) \\
    \mathcal{A} \SEM{ a_1 - a_2 } (s) & = & \mathcal{A} \SEM{ a_1 } (s) - \mathcal{A} \SEM{ a_2 } (s) \\
  \end{array}
\end{math}

\subsubsection{Semantics of Boolean Expressions}

\begin{math}
  \begin{array}{rcl}
    \mathcal{B} & : & \mathtt{Bexp} \to State \to {T} \\
    {T} & = & \{ {true}, {false} \} \\
    \mathcal{B} \SEM{ b } & : & State \to {T} \\
    \mathcal{B} \SEM{ \mathtt{true} } (s) & = & {true} \\
    \mathcal{B} \SEM{ \mathtt{false} } (s) & = & {false} \\
    \mathcal{B} \SEM{ a_1 = a_2 } (s) & = & \mathcal{A} \SEM{ a_1 } (s) = \mathcal{A} \SEM{ a_2 } (s) \\
    \mathcal{B} \SEM{ a_1 \leq a_2 } (s) & = & \mathcal{A} \SEM{ a_1 } (s) \leq \mathcal{A} \SEM{ a_2 } (s) \\
    \mathcal{B} \SEM{ \neg b } (s) & = & \mathcal{B} \SEM{ b } (s) = {false} \\
    \mathcal{B} \SEM{ b_1 \land b_2 } (s) & = & \mathcal{B} \SEM{ b_1 } (s) \wedge \mathcal{B} \SEM{ b_2 } (s) \\
  \end{array}
\end{math}

\subsubsection{Free Variables}

A set of variables occurring in the expression.

\begin{math}
  \begin{array}{rcl}
    FV(n) & = & \emptyset \\
    FV(x) & = & \{ x \} \\
    FV(a_1 + a_2) & = & FV(a_1) \cup FV(a_2) \\
    FV(a_1 \times a_2) & = & FV(a_1) \cup FV(a_2) \\
    FV(a_1 - a_2) & = & FV(a_1) \cup FV(a_2) \\
  \end{array}
\end{math}

q\paragraph{Lemma}

Let $ s $ and $ s' $ be two states satisfying that $ s(x) = s'(x) $
for all $ x \in FV(a) $.

Then, $ \mathcal{A} \SEM{ a } (s) =\mathcal{A} \SEM{ a } (s') $ holds.

\paragraph{Lemma}

Let $ s $ and $ s' $ be two states satisrying that $ s(x) = s'(x) $
for all $ x \in FV(b) $.

Then, $ \mathcal{B} \SEM{ b } (s) = \mathcal{B} \SEM{ b } (s') $
holds.


\subsubsection{Substitution}

$ a [ y \mapsto a_0 ] $ means the arithmetic expression obtained by
replacing each occurrence of $ y $ in $a $ by $ a_0 $.

\begin{math}
  \begin{array}{rcl}
    n[y \mapsto a_0 ] & = & n \\
    x[y \mapsto a_0 ] & = & \begin{cases}
      a_0 & \mathtt{if} x = y \\
      x & \mathtt{if} x \neq y \\
    \end{cases} \\
    (a_1 + a_2)[y \mapsto a_0] & = & (a_1[y \mapsto a_0]) + (a_2[y \mapsto a_0]) \\
    (a_1 \times a_2)[y \mapsto a_0] & = & (a_1[y \mapsto a_0]) \times (a_2[y \mapsto a_0]) \\
    (a_1 - a_2)[y \mapsto a_0] & = & (a_1[y \mapsto a_0]) - (a_2[y \mapsto a_0]) \\
  \end{array}
\end{math}


\begin{math}
  (s[y \mapsto v])(x) =
  \begin{cases}
    v & \mathtt{if} x = y \\
    s(x) & \mathtt{if} x \neq y \\
  \end{cases}
\end{math}



\paragraph{Lemma}
$\mathcal{A} \SEM{ a[y \mapsto a_0] } (s) = \mathcal{A} \SEM{a} (s [ y
\mapsto \mathcal{A} \SEM{a_0} (s)]) $ for alll state s.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Operational Semantics}

Also called as \textit{transitional} semantics.

Semantics of a program is defined by the computation steps executed on
a machine. In other words, operational semantics is concerned about
\textbf{how to execute} the program and not merly what the execution
result is.

The semantics is defined as a transition system
$ ( \mathbb{S}, \to ) $, where

\begin{itemize}
\item $ \mathbb{S} $ is the set of all \textbf{possible states}.
\item $ \to \subseteq \mathbb{S} \times \mathbb{S} $ is a
  \textbf{transition relation} between two states. Describes how the
  execution takes place.
\item $ s \in \mathbb{S} $ is a \textbf{state} of the program. It is
  either $ \pair{ S, s }$ which is a non-terminal state (i.e., the
  statement $ S $ is to be executed from the state $ s $), or $ s $
  which is a terminal state.
\end{itemize}

There are two approaches for operational semantics.  The difference
between the two are in the definitions of \textsl{transition
  relation}.

\subsubsection{Big-step}
\label{sec:big-step}

Big-step operational semantics describes how the \textbf{overall
  results} of executions are obtained.

The transition relation specified the relationship between the initial
state and the final state: $ \pair{ S, s } \to s' $

Transition relation is defined with \textbf{inference rules} of the
form:
\\

\begin{math}
\frac{ \pair{ S_1, s_1} \to s_1', ..., \pair{ S_n, s_n} \to s_n' }
{ \pair{S, s} \to s' }
\text{ if ... }
\end{math}

where

\begin{itemize}
\item $ S_1, ..., S_n $ are \textsl{statements} that consistute $ S $.
\item A rule has a number of premises and one conclusion.
\item A rule may also have a number of conditions that have to be
  fulfilled whenever the rule is applied.
\item Rules without premises are called \textbf{axioms}.
\end{itemize}

Big-step operational semantics for the while language is:


\begin{tabularx}{\textwidth}{c}
\\
$ \overline{ \pair{ x := a, s } \to s [ x \mapsto \mathcal{A}\SEM{a}(s) ] } $\\
\\
$ \overline{ \pair{ \mathtt{skip}, s} \to s } $\\
\\

$ \underline{ \pair{S_1, s} \to s' \text{   } \pair{S_2, s'} \to s''} $ \\
$ \pair{S_1;S_2, s} \to s'' $ \\
\\

\end{tabularx}


\paragraph{Semantic Function for Statements}

The \textbf{semantic function} for statements is the partial function:

\begin{math}
  \begin{array}{rcl}
    \mathcal{S}_b & : & Stm \to ( State \to State) \\
    \mathcal{S}_b \SEM{S} (s) & = &
                                    \begin{cases}
                                      s' & \mathtt{if} \pair{S, s} \to s' \\
                                      \mathbf{undef} & \mathtt{otherwise}
                                    \end{cases}

  \end{array}
\end{math}


\subsubsection{Small-step}
\label{sec:small-step}

Small-step operational semantics describes how the \textbf{individual
  steps} of the computations take place.

The individual computation steps are described by the transition
relation of the form:
\\

\begin{math}
  \pair{S, s} \implies \gamma
\end{math}

where $\gamma$ is either non-terminal state or terminal state. The
transition expresses the first step of the execution of $S$ from state
$s$.

\begin{itemize}
\item If $ \gamma = \pair{S', s'} $, non-terminal state, then the
  execution of $S$ from $s$ is not completed and the remaining
  computation continues with $\pair{S', s'}$.
\item If $ \gamma = s'$, terminal state, then the execution of $S$
  from $s$ has terminated and the final state is $s'$.
\end{itemize}

We say $\pair{S, s}$ is \textbf{stuck} if there is no $\gamma$ such
that $\pair{S, s} \implies \gamma$.

\paragraph{Semantic Function}

The semantic function $\mathcal{S}_s$ for small-step semantics is:
\begin{math}
  \begin{array}{rcl}
    \mathcal{S}_s & : & Stm \to (State \to State) \\
    \mathcal{S}_s \SEM{S} (s) & = &
                                    \begin{cases}
                                      s' & \mathtt{if} \pair{S, s} \xRightarrow{*} s' \\
                                      \mathbf{undef} & \mathtt{otherwise}
                                    \end{cases}

  \end{array}
\end{math}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Denotational Semantics}
Also called as \textit{compositional} semantics.

Semantics of a program is defined by the semantics of the sub-parts of
the program. Thus, proving its soundness is by structural induction on
the program. For some realistic programming languages, even defining
their compositional semantics is an obstacle, due to \texttt{goto},
\texttt{exception}, or \texttt{call}.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "program-analysis"
%%% End:
